{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef1729c-b07d-4fbe-bc92-e1b816cc5a07",
   "metadata": {},
   "source": [
    "### Analyse automatique du narratif \n",
    "`Shami THIRION SEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a66a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import os, math\n",
    "from utils import * # import des fonctions depuis le fichier utils\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#visualisation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# outils TAL en bengali\n",
    "import BnLemma as lm\n",
    "from bnlp import BengaliPOS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835a520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "banglaGanashakti = \"../corpus/txtFiles/banglaGanashakti.txt\"\n",
    "\n",
    "bartamanpatrika = \"../corpus/txtFiles/bartamanpatrika.txt\"\n",
    "anandabazar = \"../corpus/txtFiles/anandabazar.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d23dd14-3a5b-4f6b-b1cd-bae956cd2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bangla POS tagging\n",
    "bn_pos = BengaliPOS()\n",
    "from bnlp import BengaliCorpus as corpus\n",
    "stopwords = corpus.stopwords[:20] + ['রি' + 'টি']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6823fce-7d0c-4d2a-ab8f-3de11892cbd0",
   "metadata": {},
   "source": [
    "#### BanglaGanashakti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b32bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ganashakti = read_corpus(banglaGanashakti, 1000)\n",
    "filtered_bartamanpatrika = read_corpus(bartamanpatrika, 1000)\n",
    "filtered_anandabazar = read_corpus(anandabazar, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28cc5666-da07-471e-ab77-3587980463cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[],\n",
       "  ['আক্রান্ত', 'বিধান', 'বিপন্ন', 'গণতন্ত্র'],\n",
       "  [],\n",
       "  ['বিধান',\n",
       "   'প্রস্তাব',\n",
       "   'তন্ত্র',\n",
       "   'শব্দ',\n",
       "   'সরকার',\n",
       "   'বেসরকারি',\n",
       "   'হাত',\n",
       "   'পরিষ্কার'],\n",
       "  ['বিয়োজন',\n",
       "   'চেয়ে',\n",
       "   'বিজেপি',\n",
       "   'সাংসদ',\n",
       "   'সরকার',\n",
       "   'মুখ',\n",
       "   'এমন',\n",
       "   'দেশ',\n",
       "   'সামরিক']],\n",
       " [['শত',\n",
       "   'নেতৃত্ব',\n",
       "   'বিরাট',\n",
       "   'ভারত',\n",
       "   'ক্রিকেট',\n",
       "   'কন্ট্রোল',\n",
       "   'বোর্ড',\n",
       "   'প্রস্তাব',\n",
       "   'গ্রহণ',\n",
       "   'শত',\n",
       "   'টেস্ট',\n",
       "   'অধিনায়ক',\n",
       "   'বিদায়',\n",
       "   'ম্যাচ',\n",
       "   'মাইলস্টোন',\n",
       "   'ম্যাচ',\n",
       "   'কোন',\n",
       "   'চাওয়া',\n",
       "   'অভিমানী',\n",
       "   'কে'],\n",
       "  ['টেস্ট',\n",
       "   'অধিনায়ক',\n",
       "   'বিরাট',\n",
       "   'কোকিল',\n",
       "   'ইস্তফা',\n",
       "   'পাঁচ',\n",
       "   'বুমরাহকেও',\n",
       "   'বেশ',\n",
       "   'অবাক'],\n",
       "  ['হ্যামস্ট্রিংয়ে',\n",
       "   'চোট',\n",
       "   'ওয়েটিং',\n",
       "   'ইন্ডিজের',\n",
       "   'বিরুদ্ধ',\n",
       "   'সিরিজ',\n",
       "   'রোহিত',\n",
       "   'শর্করা',\n",
       "   'ফেব্রুয়ারি',\n",
       "   'গোড়ায়',\n",
       "   'ভারত'],\n",
       "  ['বিরাট', 'নতুন', 'অধিনায়ক', 'অধীনে', 'কাজ', 'কঠিন'],\n",
       "  ['ের',\n",
       "   'সপ্তম',\n",
       "   'ব্যালট',\n",
       "   'ডিও',\n",
       "   'সি',\n",
       "   'ল্যাং',\n",
       "   'বা',\n",
       "   'পায়ে',\n",
       "   'জাদু',\n",
       "   'হার']],\n",
       " [['১৫টা', 'লোক', 'ঘিরে'],\n",
       "  ['ষষ্ঠী',\n",
       "   'সপ্তম',\n",
       "   'তুলনা',\n",
       "   'রবিবার',\n",
       "   'গা',\n",
       "   'তুলনা',\n",
       "   'রাস্তা',\n",
       "   'গা',\n",
       "   'উত্তর',\n",
       "   'দক্ষিণ',\n",
       "   'নিয়ন্ত্রণ',\n",
       "   'বড়',\n",
       "   'পুজোগুলোয়',\n",
       "   'চোখ',\n",
       "   'পড়া'],\n",
       "  ['রাজ্য',\n",
       "   'বিজড়িত',\n",
       "   'ভাঙা',\n",
       "   'দশ',\n",
       "   'রাম',\n",
       "   'সুনীল',\n",
       "   'বনসাই',\n",
       "   'দল',\n",
       "   'কেন্দ্র',\n",
       "   'নেতৃত্ব',\n",
       "   'পর্যবেক্ষণ',\n",
       "   'মঙ্গল',\n",
       "   'পরিকল্পনা',\n",
       "   'পালন',\n",
       "   'নতুন',\n",
       "   'দায়িত্ব'],\n",
       "  ['কোটি', 'গুটখায়', 'রঙ', 'শহর'],\n",
       "  ['শীর্ষ', 'আদালত', 'প্রতি', 'মানুষ', 'আস্থা', 'বজায়', 'দায়িত্ব', 'কার']])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ganashakti[:5], filtered_bartamanpatrika[:5],filtered_anandabazar[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "403391ef-aaf3-495c-be81-aaffc9987b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আরএসএস', 'অন', 'যবস', 'রহসন', 'পর', 'ণত']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    single_string = [ ' '.join(line) for line in texts]\n",
    "    data_words = [gensim.utils.simple_preprocess(text) for text in single_string]\n",
    "    return data_words\n",
    "    \n",
    "data_words  = gen_words(filtered_ganashakti)\n",
    "print(data_words[9][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f0497b-ef1d-44fe-925d-e93160233f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "# # CORPORA\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "print (corpus[0][0:20])\n",
    "\n",
    "# word = id2word[[9][:1][0]]\n",
    "# print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a56ca6a-eacb-4b29-8d95-98b74b51b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BIGRAMMES ET TIGRAMMES\n",
    "\n",
    "# # filtered_docs ou data_words\n",
    "filtered_ganashakti = [list for list in filtered_ganashakti if len(list)>0]\n",
    "\n",
    "# # list_of_docs = data_words\n",
    "\n",
    "# bigram_phrases = gensim.models.Phrases(list_of_docs, min_count=5, threshold=50)\n",
    "# trigram_phrases = gensim.models.Phrases(bigram_phrases[list_of_docs], threshold=50)\n",
    "\n",
    "# bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "# trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "# # bigram, trigram\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     # print(bigram[doc] for doc in texts)\n",
    "#     return ([bigram[doc] for doc in texts])\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "\n",
    "# data_bigrams = make_bigrams(list_of_docs)\n",
    "# data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "data_bigrams_trigrams = make_bigrams_trigrams(filtered_bartamanpatrika)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adf34e9b-2ea5-40cd-8fba-d9c0097d3871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['শত', 'নেতৃত্ব', 'বিরাট', 'ভারত', 'ক্রিকেট', 'কন্ট্রোল', 'বোর্ড', 'প্রস্তাব', 'গ্রহণ', 'শত', 'টেস্ট', 'অধিনায়ক', 'বিদায়', 'ম্যাচ', 'মাইলস্টোন', 'ম্যাচ', 'কোন', 'চাওয়া', 'অভিমানী', 'কে'], ['টেস্ট', 'অধিনায়ক', 'বিরাট', 'কোকিল', 'ইস্তফা', 'পাঁচ', 'বুমরাহকেও', 'বেশ', 'অবাক'], ['হ্যামস্ট্রিংয়ে', 'চোট', 'ওয়েটিং', 'ইন্ডিজের', 'বিরুদ্ধ', 'সিরিজ', 'রোহিত', 'শর্করা', 'ফেব্রুয়ারি', 'গোড়ায়', 'ভারত'], ['বিরাট', 'নতুন', 'অধিনায়ক', 'অধীনে', 'কাজ', 'কঠিন'], ['ের', 'সপ্তম', 'ব্যালট', 'ডিও', 'সি', 'ল্যাং', 'বা', 'পায়ে', 'জাদু', 'হার'], ['ভ্যাকসিন', 'নেব', 'নোভাক', 'জকোভিচ', 'প্রশ্ন', 'বিশ্ব'], ['ভার', 'গ্যালারি', 'নোনোভাক', 'লেখা', 'একা', 'পোস্ট', 'জকোভিচ', 'অস্ট্রেলিয়ান'], ['প্রত্যাখ্যাত', 'অ্যাথলেতিক', 'বিলাত', 'গোলে', 'সুপার', 'কাপ', 'ঘর', 'রিয়াল', 'মাদ্রাসা', 'রবিবার', 'সৌদি', 'আরব'], ['আইএসএলে', 'রানার', 'মোহন', 'বাগান', 'সুবাদ', 'ব্রিগেড', 'কাপের', 'প্লেঅফে']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data_bigrams_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a43a3ff-609b-4f48-a706-f0b7cc1f78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF\n",
    "# from gensim.models import TfidfModel\n",
    "\n",
    "# id2word = Dictionary(data_bigrams_trigrams)\n",
    "# corpus = [id2word.doc2bow(text) for text in filtered_docs]\n",
    "\n",
    "# print(corpus)\n",
    "\n",
    "# tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "# low_value = 0.03\n",
    "\n",
    "# words = []\n",
    "# words_missing_in_tfidf = []\n",
    "\n",
    "# # enlève les termes trops fréquents\n",
    "# for i in range(0, len(corpus)):\n",
    "#     bow = corpus[i]\n",
    "#     low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "#     tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "#     bow_ids = [id for id, value in bow]\n",
    "#     low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "#     drops = low_value_words+words_missing_in_tfidf\n",
    "#     for item in drops:\n",
    "#         words.append(id2word[item])\n",
    "#     words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "#     new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "#     corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdff6628-c7b7-46ee-8fcf-621b0ff595ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(2, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(9, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1)],\n",
       " [(7, 1),\n",
       "  (13, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1)],\n",
       " [(26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)],\n",
       " [(2, 1),\n",
       "  (16, 1),\n",
       "  (30, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (36, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1)],\n",
       " [(13, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1)],\n",
       " [(36, 1),\n",
       "  (44, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1)],\n",
       " [(63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=get_corpus(filtered_ganashakti, data_bigrams_trigrams)\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ad2321e-9783-4fcd-89ab-834e1b0b4702",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 45 is out of bounds for axis 1 with size 45",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mldamodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLdaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mupdate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/gensim/models/ldamodel.py:521\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    519\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    520\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    524\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/gensim/models/ldamodel.py:991\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    988\u001b[0m reallen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)  \u001b[38;5;66;03m# keep track of how many documents we've processed so far\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_every \u001b[38;5;129;01mand\u001b[39;00m ((reallen \u001b[38;5;241m==\u001b[39m lencorpus) \u001b[38;5;129;01mor\u001b[39;00m ((chunk_no \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (eval_every \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumworkers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)):\n\u001b[0;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlencorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher:\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;66;03m# add the chunk to dispatcher's job queue, so workers can munch on it\u001b[39;00m\n\u001b[1;32m    995\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, dispatching documents up to #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    997\u001b[0m         pass_, chunk_no \u001b[38;5;241m*\u001b[39m chunksize \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk), lencorpus\n\u001b[1;32m    998\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/gensim/models/ldamodel.py:847\u001b[0m, in \u001b[0;36mLdaModel.log_perplexity\u001b[0;34m(self, chunk, total_docs)\u001b[0m\n\u001b[1;32m    845\u001b[0m corpus_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(cnt \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m chunk \u001b[38;5;28;01mfor\u001b[39;00m _, cnt \u001b[38;5;129;01min\u001b[39;00m document)\n\u001b[1;32m    846\u001b[0m subsample_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m total_docs \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[0;32m--> 847\u001b[0m perwordbound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsample_ratio\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (subsample_ratio \u001b[38;5;241m*\u001b[39m corpus_words)\n\u001b[1;32m    848\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m per-word bound, \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[38;5;124m perplexity estimate based on a held-out corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents with \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    850\u001b[0m     perwordbound, np\u001b[38;5;241m.\u001b[39mexp2(\u001b[38;5;241m-\u001b[39mperwordbound), \u001b[38;5;28mlen\u001b[39m(chunk), corpus_words\n\u001b[1;32m    851\u001b[0m )\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m perwordbound\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/gensim/models/ldamodel.py:1113\u001b[0m, in \u001b[0;36mLdaModel.bound\u001b[0;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbound: at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, d)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     gammad, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m     gammad \u001b[38;5;241m=\u001b[39m gamma[d]\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/gensim/models/ldamodel.py:706\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    704\u001b[0m Elogthetad \u001b[38;5;241m=\u001b[39m Elogtheta[d, :]\n\u001b[1;32m    705\u001b[0m expElogthetad \u001b[38;5;241m=\u001b[39m expElogtheta[d, :]\n\u001b[0;32m--> 706\u001b[0m expElogbetad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpElogbeta\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# phinorm is the normalizer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m    711\u001b[0m phinorm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[38;5;241m+\u001b[39m epsilon\n",
      "\u001b[0;31mIndexError\u001b[0m: index 45 is out of bounds for axis 1 with size 45"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=200,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14af31cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 45 is out of bounds for axis 1 with size 45",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[0;32m----> 2\u001b[0m vis \u001b[38;5;241m=\u001b[39m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmmds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m vis\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/pyLDAvis/gensim.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(\u001b[43m_extract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_dist\u001b[49m\u001b[43m)\u001b[49m, kwargs)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vis_prepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/pyLDAvis/gensim.py:49\u001b[0m, in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     47\u001b[0m         gamma \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39minference(corpus)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m         gamma, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     doc_topic_dists \u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m/\u001b[39m gamma\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tal-ml/lib/python3.10/site-packages/gensim/models/ldamodel.py:706\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    704\u001b[0m Elogthetad \u001b[38;5;241m=\u001b[39m Elogtheta[d, :]\n\u001b[1;32m    705\u001b[0m expElogthetad \u001b[38;5;241m=\u001b[39m expElogtheta[d, :]\n\u001b[0;32m--> 706\u001b[0m expElogbetad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpElogbeta\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# phinorm is the normalizer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m    711\u001b[0m phinorm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[38;5;241m+\u001b[39m epsilon\n",
      "\u001b[0;31mIndexError\u001b[0m: index 45 is out of bounds for axis 1 with size 45"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0a2a3-ec2b-4b20-852f-5b453b6eb550",
   "metadata": {},
   "source": [
    "#### Calcul spécificité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179198e-6413-494f-945f-a11b0c314d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higest_lowest_specificity(filtered_docs, num=20):\n",
    "\n",
    "    focus_dict = calculate_frequency(filtered_docs[:len(filtered_docs)//4])\n",
    "    window_dict= calculate_frequency(filtered_docs[len(filtered_docs)//4:])\n",
    "    \n",
    "    print(focus_dict, len(focus_dict))\n",
    "    print(window_dict, len(window_dict))\n",
    "    \n",
    "    print(f'len focus dict \\t len window dict')\n",
    "    print(len(focus_dict), len(window_dict))\n",
    "    specificite = process_specif(focus_dict, window_dict, path=\"cache/\") # dict specificite\n",
    "    \n",
    "    # affiche max et min\n",
    "    max_values = sorted(specificite.items(), key = lambda x: x[1], reverse = True)[:num]\n",
    "    min_values = sorted(specificite.items(), key = lambda x: x[1])[:num]\n",
    "    \n",
    "    print(\"\\nMoins spécifiques :\")\n",
    "    for sp in min_values:\n",
    "        print(sp)\n",
    "        \n",
    "    print(\"\\nPlus spécifiques :\")\n",
    "    for sp in sorted(max_values, key= lambda x : x[1]): \n",
    "        print(sp)\n",
    "\n",
    "    return max_values, min_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09063107-7448-4f1f-a3a4-ac5697933641",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_higest_lowest_specificity(filtered_bartamanpatrika)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb484c-dd90-470b-bf2e-82bd0d6590b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d1aab-6f11-4e9f-a808-23c89877c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(banglaGanashakti, 'r')\n",
    "lines = data.readlines(1000) # lecture par ligne\n",
    "lines = [line for line in lines if not line.startswith('Image')]\n",
    "# print(lines[:10])\n",
    "docs = [line.split() for line in lines]\n",
    "docs[:10]\n",
    "\n",
    "filtered_docs = filter_doc(docs)\n",
    "\n",
    "for line, filtered_line in zip(docs[:5], filtered_docs[:5]):\n",
    "    print(f'line = {line}, length = {len(line)}')\n",
    "    print(f'filtered tokens = {filtered_line}, length = {len(filtered_line)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2edc499-a28f-43f3-8bb4-67be608bfaa9",
   "metadata": {},
   "source": [
    "### Bartamanpatrika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcdc31-b206-452a-9110-a9035cdf4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(bartamanpatrika, 'r')\n",
    "lines = data.readlines() # lecture par ligne\n",
    "lines = [line for line in lines if not line.startswith('Image')]\n",
    "# print(lines[:10])\n",
    "docs = [line.split() for line in lines]\n",
    "docs[:10]\n",
    "\n",
    "filtered_docs = filter_doc(docs)\n",
    "\n",
    "for line, filtered_line in zip(docs[:5], filtered_docs[:5]):\n",
    "    print(f'line = {line}, length = {len(line)}')\n",
    "    print(f'filtered tokens = {filtered_line}, length = {len(filtered_line)}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a0202-75f2-4843-ac6c-13541a840d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?\n",
    "def gen_words(texts):\n",
    "    data_words = [gensim.utils.simple_preprocess(text) for text in texts]\n",
    "    return data_words\n",
    "\n",
    "# single\n",
    "single_string = [ ' '.join(line) for line in filtered_docs]\n",
    "print(single_string[0][:100])\n",
    "# for line in filtered_docs:\n",
    "    \n",
    "data_words  = gen_words(single_string)\n",
    "print(data_words[0][:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950fba5-d537-488c-98b8-d4e03a15121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORPORA\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "print (corpus[0][0:20])\n",
    "\n",
    "word = id2word[[0][:1][0]]\n",
    "print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96ecdc-d413-454d-b0bd-8fd9137ef9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BIGRAMMES ET TRIGRAMMES\n",
    "\n",
    "# filtered_docs ou data_words\n",
    "list_of_docs = [list for list in filtered_docs if len(list)>0]\n",
    "# list_of_docs = data_words\n",
    "\n",
    "bigram_phrases = gensim.models.Phrases(list_of_docs, min_count=5, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[list_of_docs], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "# bigram, trigram\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    # print(bigram[doc] for doc in texts)\n",
    "    return ([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "\n",
    "data_bigrams = make_bigrams(list_of_docs)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9cb5c3-89e1-4126-ba0c-a5a1962c572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data_bigrams_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13a855-5352-4353-888b-3235ac203520",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = Dictionary(data_bigrams_trigrams)\n",
    "corpus = [id2word.doc2bow(text) for text in filtered_docs]\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "low_value = 0.03\n",
    "\n",
    "words = []\n",
    "words_missing_in_tfidf = []\n",
    "\n",
    "# enlève les termes trops fréquents\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350534cd-c5b0-4c91-8490-0de837435fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0c5d6-1b76-482d-be0a-3eb340402e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2ef63-4312-480a-9063-e23836644de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ded5464-089f-4309-86d6-6602e3609864",
   "metadata": {},
   "source": [
    "### Anandabazar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337e0c9-6cbf-442d-9fc5-d2ce1a8978b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
