{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import os, math\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import BnLemma as lm\n",
    "from bnlp import BengaliPOS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835a520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anandabazar = \"../corpus/txtFiles/anandabazar.txt\"\n",
    "bartaman = \"../corpus/txtFiles/bartamanpatrika.txt\"\n",
    "banglaGanashakti = \"../corpus/txtFiles/banglaGanashakti.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d23dd14-3a5b-4f6b-b1cd-bae956cd2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bangla POS tagging\n",
    "bn_pos = BengaliPOS()\n",
    "\n",
    "from bnlp import BengaliCorpus as corpus\n",
    "stopwords = corpus.stopwords[:20] + ['রি' + 'টি']\n",
    "# stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44d54a0b-fce5-4f86-ad64-862d0c6e926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_doc(texts):\n",
    "#     filtered = []\n",
    "#     for line in texts:\n",
    "#         filtered_line = [token for token in line if bn_pos.tag(token)[0][1] in ['NC','NP', 'JQ', 'JJ']] # 'NST',, 'AMN', , 'NCgen'\n",
    "#         filtered_line = [token for token in filtered_line if token not in stopwords]\n",
    "#         if len(filtered_line) > 0:\n",
    "#             filtered.append(filtered_line)\n",
    "                   \n",
    "#     return filtered    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4c84825-cf77-4c0d-8779-d11396a3a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_bangla(text):\n",
    "    if text is not None:\n",
    "        lemma = lm.Lemmatizer()\n",
    "        try:\n",
    "            lemmatized = lemma.lemma(text)\n",
    "            # print(lemmatized)\n",
    "            return lemmatized.split()\n",
    "        except Exception as e:\n",
    "            # print(f\"Error in lemmatizing text: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "def filter_doc(texts):  \n",
    "    filtered = []\n",
    "    for line in texts:\n",
    "            # print(line)\n",
    "            \n",
    "            # print(line)\n",
    "            filtered_line = [token for token in line if bn_pos.tag(token)[0][1] in ['NC','NP', 'JQ', 'JJ']] # 'NST',, 'AMN', , 'NCgen'\n",
    "            filtered_line = [token for token in filtered_line if token not in stopwords]\n",
    "            line = lemmatize_bangla(' '.join(filtered_line))\n",
    "            if line is not None:\n",
    "                filtered.append(line)\n",
    "                   \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b32bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(banglaGanashakti, 'r')\n",
    "lines = data.readlines()[:10000] # lecture par ligne\n",
    "# print(lines[:10])\n",
    "docs = [line.split() for line in lines]\n",
    "docs[:10]\n",
    "\n",
    "filtered_docs = filter_doc(docs)\n",
    "\n",
    "\n",
    "# pprint(docs[:10])\n",
    "# pprint(filtered_docs[:10])\n",
    "\n",
    "for line, filtered_line in zip(docs[:10], filtered_docs[:10]):\n",
    "    print(f'line = {line}, length = {len(line)}')\n",
    "    print(f'filtered tokens = {filtered_line}, length = {len(filtered_line)}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66addc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2word\n",
    "# id2word = corpora.Dictionary(docs) # docs = List[List[str]]\n",
    "# corpus = [id2word.doc2bow(text) for text in filtered_docs]\n",
    "\n",
    "# print(corpus)[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403391ef-aaf3-495c-be81-aaffc9987b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?\n",
    "def gen_words(texts):\n",
    "    data_words = [gensim.utils.simple_preprocess(text) for text in texts]\n",
    "    return data_words\n",
    "\n",
    "# single\n",
    "single_string = [ ' '.join(line) for line in filtered_docs]\n",
    "print(single_string[0][:100])\n",
    "# for line in filtered_docs:\n",
    "    \n",
    "data_words  = gen_words(single_string)\n",
    "print(data_words[9][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56ca6a-eacb-4b29-8d95-98b74b51b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BIGRAMMES ET TIGRAMMES\n",
    "\n",
    "# filtered_docs ou data_words\n",
    "\n",
    "bigram_phrases = gensim.models.Phrases(filtered_docs, min_count=5, threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[filtered_docs], threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "# bigram, trigram\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    # print(bigram[doc] for doc in texts)\n",
    "    return ([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "\n",
    "data_bigrams = make_bigrams(filtered_docs)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf34e9b-2ea5-40cd-8fba-d9c0097d3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data_bigrams_trigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43a3ff-609b-4f48-a706-f0b7cc1f78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = Dictionary(data_bigrams_trigrams)\n",
    "corpus = [id2word.doc2bow(text) for text in filtered_docs]\n",
    "\n",
    "print(corpus[0][:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "low_value = 0.03\n",
    "\n",
    "words = []\n",
    "words_missing_in_tfidf = []\n",
    "\n",
    "# enlève les termes trops fréquents\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6ad1b-ba18-45eb-bf45-f136517e3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [id2word.doc2bow(text) for text in filtered_docs]\n",
    "# print (corpus[0][0:20]) \n",
    "\n",
    "# word = id2word[[9][:1][0]]\n",
    "# print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2321e-9783-4fcd-89ab-834e1b0b4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=7,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14af31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0a2a3-ec2b-4b20-852f-5b453b6eb550",
   "metadata": {},
   "source": [
    "#### Calcul spécificité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf57989-cd57-47eb-9b8d-ad492a3592b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def calculate_frequency(texts):\n",
    "    frequency = defaultdict(int)    \n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            if bn_pos.tag(token)[0][1] in ['NC','NP', 'JQ', 'JJ'] :\n",
    "                frequency[token] +=1\n",
    "                \n",
    "    return dict(frequency)\n",
    "\n",
    "\n",
    "\n",
    "def specif(f, F, t, T):\n",
    "    try:\n",
    "        r_command = f'R --vanilla -s -e \\'library(\"textometry\", lib=\".\"); \\\n",
    "                    res <- specificities.distribution.plot({f},{F},{t},{T}); print(res[\"mode\"]); \\\n",
    "                    print(res[\"pfsum\"][[1]][[{f}+1]]);\\''\n",
    "        result = subprocess.check_output(r_command, shell=True, text=True).split('\\n') # appel R\n",
    "        mode, proba = str(result[1]).split()[1], str(result[3]).split()[1]\n",
    "        if is_float(mode) and is_float(proba):\n",
    "            mode, proba  = float(mode), float(proba)\n",
    "            try:\n",
    "                if mode <= f:\n",
    "                    specificite = math.fabs(math.log10(math.fabs(proba))) \n",
    "                else :\n",
    "                    specificite = -math.fabs(math.log10(math.fabs(proba))) \n",
    "                return specificite\n",
    "            except ValueError:\n",
    "                return None\n",
    "    except subprocess.CalledProcessError as e:\n",
    "            print(f\" {e}.\\nErreur en calculant la spécificité  pour : <{f}>, <{F}>, <{t}>, <{T}>\")\n",
    "            return None\n",
    "    \n",
    "\n",
    "## calcule f F t T -> appelle specif ##\n",
    "def process_specif(focus_freq, window_freq, path):\n",
    "    # if path exists else create it\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    F = sum(focus_freq.values())\n",
    "    T = sum(window_freq.values())\n",
    "    spec_dict={}\n",
    "    for key in focus_freq.keys():\n",
    "        f = focus_freq[key]\n",
    "        if key in window_freq.keys():\n",
    "            t = window_freq[key]\n",
    "        # on exlut les t=0\n",
    "            specificite = specif(f, F, t, T) # specificite pour mot=key\n",
    "            if specificite is not None:\n",
    "                spec_dict[key] = specificite\n",
    "                filename = path + f\"/specif-{f}-{F}-{t}-{T}.txt\" \n",
    "                with open(filename, 'w') as file:\n",
    "                    file.write(f'{key}\\t{specificite}\\n')\n",
    "        \n",
    "    return spec_dict      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179198e-6413-494f-945f-a11b0c314d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# banglaGanashakti = '../corpus/txtFiles/banglaGanashakti.txt'\n",
    "# anandabazar = '../corpus/txtFiles//anandabazar.txt'\n",
    "\n",
    "data = open(anandabazar, 'r')\n",
    "lines = data.readlines() # lecture par ligne\n",
    "# print(lines[:10])\n",
    "docs = [line.split() for line in lines]\n",
    "docs[:10]\n",
    "filtered_docs = filter_doc(docs)\n",
    "focus_dict = calculate_frequency(filtered_docs[:len(filtered_docs)//4])\n",
    "window_dict= calculate_frequency(filtered_docs[len(filtered_docs)//4:])\n",
    "\n",
    "# print(focus_dict, len(focus_dict))\n",
    "# print(window_dict, len(window_dict))\n",
    "print(len(focus_dict), len(window_dict))\n",
    "specificite = process_specif(focus_dict, window_dict, path=\"cache/\") # dict specificite\n",
    "\n",
    "# affiche max et min\n",
    "max_values = sorted(specificite.items(), key = lambda x: x[1], reverse = True)[:20]\n",
    "min_values = sorted(specificite.items(), key = lambda x: x[1])[:20]\n",
    "\n",
    "print(\"\\nMoins spécifiques :\")\n",
    "for sp in min_values:\n",
    "    print(sp)\n",
    "    \n",
    "print(\"\\nPlus spécifiques :\")\n",
    "for sp in sorted(max_values, key= lambda x : x[1]): \n",
    "    print(sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261f243-86d5-4042-899e-389747285471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_frequency(text):\n",
    "#     frequency = defaultdict(int)    \n",
    "#     for token in doc:\n",
    "#         if bn_pos.tag(token)[0][1] in ['NC','NP', 'JQ', 'JJ'] :\n",
    "#             frequency[token] +=1\n",
    "                \n",
    "#     return dict(frequency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220b65e-9f05-44ca-8b96-27a3dbd95386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94dec72-99d4-45da-a86d-9193ecef4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def specif(f, F, t, T):\n",
    "#     try:\n",
    "#         r_command = f'R --vanilla -s -e \\'library(\"textometry\", lib=\".\"); \\\n",
    "#                     res <- specificities.distribution.plot({f},{F},{t},{T}); print(res[\"mode\"]); \\\n",
    "#                     print(res[\"pfsum\"][[1]][[{f}+1]]);\\''\n",
    "#         result = subprocess.check_output(r_command, shell=True, text=True).split('\\n') # appel R\n",
    "#         mode, proba = str(result[1]).split()[1], str(result[3]).split()[1]\n",
    "#         if is_float(mode) and is_float(proba):\n",
    "#             mode, proba  = float(mode), float(proba)\n",
    "#             try:\n",
    "#                 if mode <= f:\n",
    "#                     specificite = math.fabs(math.log10(math.fabs(proba))) \n",
    "#                 else :\n",
    "#                     specificite = -math.fabs(math.log10(math.fabs(proba))) \n",
    "#                 return specificite\n",
    "#             except ValueError:\n",
    "#                 return None\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#             print(f\" {e}.\\nErreur en calculant la spécificité  pour : <{f}>, <{F}>, <{t}>, <{T}>\")\n",
    "#             return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b94f8-3710-46c8-9f08-a52ea7f21a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## calcule f F t T -> appelle specif ##\n",
    "# def process_specif(focus_freq, window_freq, path):\n",
    "#     F = sum(focus_freq.values())\n",
    "#     T = sum(window_freq.values())\n",
    "#     spec_dict={}\n",
    "#     for key in focus_freq.keys():\n",
    "#         f = focus_freq[key]\n",
    "#         if key in window_freq.keys():\n",
    "#             t = window_freq[key]\n",
    "#         # on exlut les t=0\n",
    "#             specificite = specif(f, F, t, T) # specificite pour mot=key\n",
    "#             if specificite is not None:\n",
    "#                 spec_dict[key] = specificite\n",
    "#                 filename = path + f\"/specif-{f}-{F}-{t}-{T}.txt\" \n",
    "#                 with open(filename, 'w') as file:\n",
    "#                     file.write(f'{key}\\t{specificite}\\n')\n",
    "        \n",
    "#     return spec_dict               \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1585e64-1b3a-4ee0-9062-4162e4b055b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus_dict = (calculate_frequency(focus_corpus)) # focus_corpus = [\n",
    "# window_dict = calculate_frequency(window_corpus)\n",
    "# specificite = process_specif(focus_dict, window_dict, path=\"cache/\") # dict specificite\n",
    "\n",
    "# # affiche max et min\n",
    "# max_values = sorted(specificite.items(), key = lambda x: x[1], reverse = True)[:10]\n",
    "# min_values = sorted(specificite.items(), key = lambda x: x[1])[:10]\n",
    "\n",
    "# print(\"\\nMoins spécifiques :\")\n",
    "# for sp in min_values:\n",
    "#     print(sp)\n",
    "    \n",
    "# print(\"\\nPlus spécifiques :\")\n",
    "# for sp in sorted(max_values, key= lambda x : x[1]): \n",
    "#     print(sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a0dec3-a592-4ed2-9925-4cda0f6480f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
